# ðŸ§  Modular RAG PDF Chatbot with FastAPI, ChromaDB & Streamlit

## ðŸš€ **NOW WITH LATEST GEMINI MODELS!**
### âš¡ Featuring Gemini 2.0 Flash, Experimental 1206, and Enhanced Precision

## ðŸŽ¥ Watch the Tutorial

[![Watch the video](assets/ragbot2.0.png)](https://youtu.be/TxtK6NUUklQ)

This project is a cutting-edge **Retrieval-Augmented Generation (RAG)** application that allows users to upload PDF documents and chat with an AI assistant powered by the **latest Gemini models**. It features a microservice architecture with a decoupled **FastAPI backend** and **Streamlit frontend**, using **ChromaDB** as the vector store and **Google's latest Gemini models** for superior performance and precision.

### ðŸ”¥ Latest Features
- **Latest Gemini Models**: Gemini 2.0 Flash, Experimental 1206, 1121, and more
- **Enhanced Precision**: Configurable temperature and advanced parameters
- **OCR Support**: Process both text-based and scanned PDFs
- **Model Selection UI**: Choose the best model for your use case
- **Real-time Performance**: Lightning-fast responses with latest models

---

## ðŸ“‚ Project Structure

```
ragbot2.0/
â”œâ”€â”€ client/         # Streamlit Frontend
â”‚   |â”€â”€components/
|   |  |â”€â”€chatUI.py
|   |  |â”€â”€history_download.py
|   |  |â”€â”€upload.py
|   |â”€â”€utils/
|   |  |â”€â”€api.py
|   |â”€â”€app.py
|   |â”€â”€config.py
â”œâ”€â”€ server/         # FastAPI Backend
â”‚   â”œâ”€â”€ chroma_store/ ....after run
|   |â”€â”€modules/
â”‚      â”œâ”€â”€ load_vectorestore.py
â”‚      â”œâ”€â”€ llm.py
â”‚      â”œâ”€â”€ pdf_handler.py
â”‚      â”œâ”€â”€ query_handlers.py
|   |â”€â”€uploaded_pdfs/ ....after run
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ main.py
â””â”€â”€ README.md
```

---

## âœ¨ Features

- ðŸ“„ Upload and parse PDFs
- ðŸ§  Embed document chunks with HuggingFace embeddings
- ðŸ’‚ï¸ Store embeddings in ChromaDB
- ðŸ’¬ Query documents using LLaMA3 via Groq
- ðŸŒ Microservice architecture (Streamlit client + FastAPI server)

---

## ðŸŽ“ How RAG Works

Retrieval-Augmented Generation (RAG) enhances LLMs by injecting external knowledge. Instead of relying solely on pre-trained data, the model retrieves relevant information from a vector database (like ChromaDB) and uses it to generate accurate, context-aware responses.

---

## ðŸ“Š Application Diagram

ðŸ“„ [Download the Full Architecture PDF](assets/ragbot2.0.pdf)

---

## ðŸš€ Getting Started Locally

### 1. Clone the Repository

```bash
git clone https://github.com/snsupratim/ragbot.git
cd ragbot
```

### 2. Setup the Backend (FastAPI)

```bash
cd server
python3 -m venv venv
source venv/bin/activate
pip3 install -r requirements.txt

# Set your Groq API Key
echo "GROQ_API_KEY=your_key_here" > .env

# Run the FastAPI server
uvicorn main:app --reload
```

### 3. Setup the Frontend (Streamlit)

```bash
cd client
python3 -m venv venv
source venv/bin/activate
pip3 install -r requirements.txt
streamlit run app.py
```

### 4. Run the Application

```bash
cd server
python3 -m venv venv
source venv/bin/activate
uvicorn main:app --reload
```

```bash
cd client
python3 -m venv venv
source venv/bin/activate
streamlit run app.py
```
Open your browser and navigate to [http://localhost:8501]